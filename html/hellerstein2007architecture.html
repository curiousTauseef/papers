<!DOCTYPE html>
<html>
<head>
  <title>Papers</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Papers</a>
  </div>
  <div id="container">
<h2 id="architecture-of-a-database-system-2007"><a href="https://scholar.google.com/scholar?cluster=11466590537214723805&amp;hl=en&amp;as_sdt=0,5">Architecture of a Database System (2007)</a></h2>
<p><strong>Chapter 1 -- Introduction.</strong> Database textbooks often focus on data structures and algorithms in the context of a single database component. This paper, as opposed most database textbooks, focuses instead on <em>database architecture</em>: the design and best practices of modern databases that are otherwise undocumented or exist only as tribal knowledge.</p>
<p>Modern relational database management systems (RDBMS) comprise five components, most of which are discussed in detail in this paper:</p>
<ol style="list-style-type: decimal">
<li>The <em>client communications manager</em> is responsible for interfacing with client connections.</li>
<li>The <em>process manager</em> is responsible allocating workers to requests using some combination of OS processes, OS threads, and user-level threads. It is also responsible for <em>admission control</em>: the process by which request processing is delayed until sufficient resources are available.</li>
<li>The <em>relational query processor</em> is responsible for translating a SQL query into an optimal query plan. It is also responsible for authorization.</li>
<li>The <em>transactional storage manager</em> implements the data structures and algorithms to store and read data from disk. It is also responsible for managing concurrent transactions. It includes the buffer manager, the lock manager, and the log manager.</li>
<li>Finally, there are miscellaneous <em>shared components and utilities</em>.</li>
</ol>
<p><strong>Chapter 2 -- Process Models.</strong> Database management systems have to handle multiple user requests concurrently. The process manager is responsible for mapping logical DBMS workers, which handle a DBMS client requests, to OS processes, OS threads, user-level threads, or some combination of the three. To simplify matters, this chapter discusses process models only for unikernels. There are three main process models:</p>
<ol style="list-style-type: decimal">
<li><em>Process per DBMS worker</em>. In this model, a new process is spawned for every request. This method is easy to implement and provides a small amount of isolation (e.g. a memory overflow in one process won't crash another process). However, this model is complicated by shared in-memory data structures such as the buffer pool and lock table. These are traditionally shared through OS supported shared memory. Moreover, context switching between processes is more expensive than context switching between threads. IBM DB2, PostgreSQL, and Oracle use this process model.</li>
<li><em>Thread per DBMS worker</em>. In this model, a new thread (OS or user-level) is spawned for every request. This model can handle more requests than the previous model, and shared in-memory data structures (e.g. buffer pool, log tail) can simply reside in the heap. However, it is less portable and harder to implement and debug. IBM DB2, Microsoft SQL Server, MySQL, Informix, and Sybase use this process model.</li>
<li><em>Process pool</em>. In this model, requests are dispatched to a fixed number of processes; a new process is not spawned for every request. This approach has all the benefits of the process per DBMS worker approach but with much less overhead.</li>
</ol>
<p>The process manager is also responsible for admission control: the process by which a request is not serviced until sufficient resources are available. Without admission control, a database can start thrashing; for example, imagine a situation in which the working set of the database is larger than the buffer pool, and all I/Os become cache misses. Admission control provides <em>graceful degradation</em>; throughput should not decrease, and latency should scale proportionally with the number of requests. Typically a two-tier admission control policy is used:</p>
<ul>
<li>First, the client communication manager limits the number of concurrently open connections.</li>
<li><p>Second, the execution admission controller runs after a query has been planned and delays execution until there are enough resources available to satisfy the query optimizer's estimated</p>
<ul>
<li>disk access and number of I/Os,</li>
<li>CPU load, and</li>
<li>memory footprint.</li>
</ul></li>
</ul>
<p>The memory footprint is particularly important because it is most commonly the cause of thrashing.</p>
<p><strong>Chapter 3 -- Parallel Architecture: Processes and Memory Coordination.</strong> Parallel hardware is ubiquitous. This chapter builds off the previous and explores process models for database systems with multiple cores and multiple machines.</p>
<ol style="list-style-type: decimal">
<li><em>Shared Memory.</em> In a shared memory model, all processors can access shared RAM and disk with roughly the same performance. All three of the process models presented in the last chapter (i.e. process-per-worker, thread-per-worker, and process-pool/thread-pool) work well in a shared memory model. The OS transparently schedules processes and threads across the processors taking advantage of the parallel hardware without much effort. It is much more difficult to modify the query evaluator to take advantage of the multiple processors. Also, systems employing user-level threading must implement thread migration to take full advantage of multiple cores.</li>
<li><p><em>Shared-Nothing.</em> A shared-nothing system is a networked cluster of independent machines that share, well, nothing. In a shared-nothing system, all coordination and communication is left to the DBMS. Typically, tables are <em>horizontally partitioned</em> between machines. That is, each machine is assigned a subset of the tuples in each table using range partitioning, hash partitioning, round-robin partitioning, or some sort of hybrid partitioning. Each machine uses a shared memory model and receives queries, creates query plans, and execute queries as usual. The big difference is that queries are now evaluated on multiple machines at once, and the DBMS has to orchestrate the exchange of control and data messages. The database also has to implement very challenging distributed protocols like distributed deadlock detection and two-phase commit. Worse, by virtue of being a distributed system, shared-nothing architectures can experience <em>partial failure</em> which can be handled in one of many ways.</p>
<ol style="list-style-type: decimal">
<li>Every machine can be stopped whenever any machine fails. This makes a shared-nothing architecture as fault-tolerant as a shared-memory architecture.</li>
<li>Some database systems, like Informix, simply skip over data hosted by a failed machine. This has weak and unpredictable semantics.</li>
<li>Data can be replicated to tolerate failures. For example, a full database backup could be maintained, or more sophisticated techniques like <a href="https://scholar.google.com/scholar?cluster=10345968159835311656&amp;hl=en&amp;as_sdt=0,5">chained declustering</a> could be employed.</li>
</ol></li>
</ol>
<p>Despite the complexities that arise from a shared-nothing architecture, they achieve unparalleled scalability. 3. <em>Shared Disk.</em> In a shared disk system, processors all have access to a shared disk with roughly equal performance; they do not share RAM. Shared disk systems are much less complicated than shared-nothing systems because the failure of any machine does not lead to data unavailability. Still, shared disk systems require explicit coordination for in-memory data sharing including distributed lock managers, distributed buffer pools, etc. 4. <em>NUMA.</em> NUMA systems provide a shared memory model over a cluster of independent shared-nothing machines. NUMA clusters are dead, but the idea of non-uniform memory access lives on in shared-memory multiprocessors. In order to scale to a large number of cores, shared-memory NUMA processors organize CPUs and memories into pods where intra-pod memory access is fast but inter-pod memory access is slow. Databases may be able to ignore the non-uniformity of a NUMA multiprocessor, or they can employ certain optimizations:</p>
<pre><code>- Memory should always be local to a processor.
- Tasks should be scheduled on the same processor it was previously run.</code></pre>
<p>Almost all databases support shared memory systems, and most support either shared-disk or shared-nothing architectures as well.</p>
<p><strong>Chapter 4 -- Relational Query Processor.</strong> The <em>relational query processor</em> is responsible for converting a textual query into an optimized dataflow query plan that's ready to be executed.</p>
<p>The first step in processing a query is that of <em>query parsing and authorization</em>. The query parser must check the syntactic well-formedness of a textual query, convert the text into an internal query representation, type check the query by resolving table and column references against information in the catalog, and perform any necessary authorization. Certain forms of authorization must be deferred to query execution. For example, a database may restrict a user's access to tuples from a table that satisfy a certain predicate. This <em>row-level security</em> depends on the values of the tuples and must be deferred to query execution. In fact, some authorization which <em>could</em> be performed during query parsing is deferred anyway. For example, deferring authorization checks to execution time allows for queries to be cached and reused between multiple clients with varying privileges.</p>
<p>Next, a query processor performs <em>query rewrites</em>: logical transformations that <em>simplify</em> and <em>normalize</em> a query without altering its semantics. Query rewrites include:</p>
<ul>
<li><em>View expansion.</em> View references in a query must be iteratively unwrapped until the final query includes only base table references.</li>
<li><em>Constant folding.</em> Expressions like <code>1 + R.a + 2 &gt; 3</code> can be simplified to <code>R.a &gt; 0</code>.</li>
<li><em>Logical predicate rewrites.</em> A query processor can sometimes deduce that a collection of predicates is unsatisfiable (e.g. <code>R.a &lt; 0 AND R.a &gt; 10</code>). Unsatisfiable predicates can be replaced with <code>FALSE</code> which enable further simplifications and optimizations. In some distributed databases that horizontally partition tables, predicates can be used to reduce the number of servers that are contacted. For example, if a server is responsible for a partition of a table <code>R</code> for all tuples where <code>0 &lt; R.a &lt; 100</code>, then it need not be contacted for a query like <code>SELECT R.a FROM R WHERE R.a &gt; 10000</code>. Finally, certain <em>transitive predicates</em> can be deduced. For example, the predicates <code>R.a = S.b AND S.b = 100</code> imply <code>R.a = 100</code>.</li>
<li><p><em>Semantic optimization.</em> Using semantic information from the database catalog can be used to further simplify queries. For example, consider an <code>Employee</code> relation that has a foreign key into a <code>Department</code> relation. With this information, the query</p>
<pre><code>SELECT E.name
FROM Emp E, Department D
WHERE E.deptno = D.deptno</code></pre></li>
</ul>
<p>can be simplified to</p>
<pre><code>    SELECT E.name
    FROM Emp E</code></pre>
<ul>
<li><em>Subquery flattening.</em> Query optimizers are so complicated that they often narrow their scope to operate only on SELECT-FROM-WHERE blocks. To enable as many optimizations as possible, queries are often canonicalized and subqueries are flattened when possible.</li>
</ul>
<p>Finally, a query is <em>optimized</em>. System R compiled queries into executable code. Later, System R developers regarded this as a mistake. Ingres compiled queries into interpretable dataflow diagrams. Later, Ingres developers somewhat ironically also regarded this as a mistake. Both compilation targets have their merits, but most modern databases have gone the way of Ingres to ensure portability. Typically, this involves optimizing individual SELECT-FROM-WHERE blocks into relational operator trees before stitching them all together. Optimizations involve:</p>
<ul>
<li><em>Plan space.</em> System R only considered left-deep query plans and deferred all Cartesian products to the top of the plan. Modern databases sometimes consider bushier trees with Cartesian products lower in the tree.</li>
<li><em>Selectivity estimation.</em> System R's selectivity estimation was based solely on relation and index cardinalities and is considered naive by today's standards. Modern databases summarize data distributions using histograms and other sketching data structures and use sampling to avoid expensive statistics computations. Moreover, algorithms like histogram joining improve selectivity estimation for joins.</li>
<li><em>Search algorithms.</em> In addition to System R's dynamic programming bottom-up approach, other databases have explored top-down approaches. Both have proven successful.</li>
<li><em>Parallelism.</em> In addition to inter-query parallelism, databases often implement intra-query parallelism. In a <em>two-phase approach</em>, the best single-node query plan is formed in one phase and then parallelized in a second phase. In a <em>one-phase approach</em>, the optimizer takes cluster information into account during optimization to try and find an optimal distributed plan. It's questionable whether the performance benefits of a two-phase plan warrant its complexity.</li>
<li><em>Auto-tuning.</em> Some databases use query traces to automatically tune the databases by, for example, suggesting new indexes to include.</li>
</ul>
<p>Query optimizers also have to deal with query caching and recompilation. Many databases allow for queries to be parsed, compiled, and stored ahead of time. These <em>prepared</em> queries can also include placeholders that are filled in at runtime. Prepared statements occasionally need to be re-optimized when, for example, an index is dropped. Certain databases avoid re-optimization to ensure predictability over performance; others aggressively re-optimize to ensure the best performance. Prepared queries can improve the performance of an application, but preparing queries ahead of time can be burdensome. Consequently, databases also support query caching to reuse (parts of) queries without necessitating ahead-of-time preparation.</p>
<p>Once a query is parsed, rewritten, and optimized into a dataflow plan, it must be executed. Typically, query plans are implemented as a tree of iterators with <em>exchange nodes</em> thrown in for parallelism. These iterators typically operate over <em>tuple references</em>: tuples of tuple pointers and column offsets. The actual tuple data is either stored in the buffer pool (BP-tuples) or copied from the buffer pool into the heap (M-tuples). Using BP-tuples avoids copies but is hard to implement correctly and may lead to a page being pinned in the buffer pool for prohibitively long. Using M-tuples can lead to unnecessary copies but is much simpler to implement.</p>
<p>Data modification statements (e.g. INSERT, UPDATE, etc) are typically compiled into simple linear dataflow diagrams. However, care must be taken to avoid things like the <em>Halloween problem</em> in which updates invalidate the index iterators used to perform the updates.</p>
<p><strong>Chapter 5 -- Storage Management.</strong> TODO</p>
<p><strong>Chapter 6 -- Transactions: Concurrency Control and Recovery.</strong> TODO</p>
<p><strong>Chapter 7 -- Shared Components.</strong> TODO</p>
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-90310997-2', 'auto');
		ga('send', 'pageview');
	</script>
</body>
</html>
