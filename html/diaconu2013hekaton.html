<!DOCTYPE html>
<html>
<head>
  <title>Papers</title>
  <link href='../css/style.css' rel='stylesheet'>
  <meta name=viewport content="width=device-width, initial-scale=1">
</head>

<body>
  <div id=header>
    <a href="../">Papers</a>
  </div>
  <div id="container">
<h1 id="hekaton-sql-servers-memory-optimized-oltp-engine-2013"><a href="https://scholar.google.com/scholar?cluster=14161764654889427045">Hekaton: SQL Server's Memory-Optimized OLTP Engine (2013)</a></h1>
<p>Hekaton is a fast in-memory OLTP engine integrated with Microsoft SQL Server. Hekaton accesses data via lock-free indexes and compiled stored procedures. Hekaton also implements optimistic multiversion concurrency control as well as parallel recovery and garbage collection procedures.</p>
<h2 id="design-considerations">Design Considerations</h2>
<ol style="list-style-type: decimal">
<li><strong>Optimize indexes for main memory.</strong> Hekaton uses lock-free hash indexes and Bw-tree indexes as opposed to more traditional indexes which were designed for disks.</li>
<li><strong>Eliminate latches and locks.</strong> Lock freedom and optimistic concurrency control allow for fast transactions.</li>
<li><strong>Compile stored procedures.</strong> Stored procedures that exclusively read Hekaton tables are compiled to C code.</li>
<li><strong>Don't partition data.</strong> If everything fits in memory on one machine, things will run much faster than if data is partitioned.</li>
</ol>
<h2 id="storage-and-indexing">Storage and Indexing</h2>
<p>Hekaton stores tuples in-memory row-by-row. Each physical tuple contains three segments:</p>
<ol style="list-style-type: decimal">
<li><strong>A header with logical timestamps.</strong> Because Hekaton implements multiversion concurrency control, each tuple is annotated with a logical begin and end timestamp.</li>
<li><strong>Links for indexes.</strong> Hekaton uses lock-free hash indexes and Bw-tree indexes. Multiple tuple versions that share the same index key are linked together.</li>
<li><strong>The tuple's data.</strong> Duh.</li>
</ol>
<p>Reads are issued at particular logical time and scan through all index entries matching the specified key. Though all entries are scanned, only those whose begin and end timestamp envelope the read timestamp are read.</p>
<p>When a transaction deletes a tuple, it temporarily writes its transaction id into the tuples end timestamp. Similarly, when a transaction inserts a tuple, it temporarily writes its transaction id in the begin timestamp. Once the transaction completes, it overwrites its id with its end timestamp. An update is modelled as a deletion followed by an insertion.</p>
<h2 id="programmability-and-query-processing">Programmability and Query Processing</h2>
<p>Hekaton compiles stored procedures to C code, then compiles and links the C code into Hekaton. The compilation re-uses a lot of existing SQL Server components. First, the stored procedure is run through SQL Server's query optimizer into a <strong>mixed abstract syntax tree</strong> (MAT). A MAT can encode metadata, imperative code, expressions, and query plans. The MAT is then compiled into lower level <strong>pure imperative tree</strong> (PIT) that is easier to compile to C.</p>
<p>Hekaton does not compile query plans into a series of function calls. Instead, a query plan is compiled into a single function and operators are connected together via labels and gotos. This allows the code to bypass some otherwise unnecessary function calls. For example, when the query is initially executed, it jumps immediately to the leaves of the query plan rather than recursively calling down to them. Some code (e.g. sort and complicated arithmetic functions) is not generated.</p>
<p>Hekaton stored procedures have some restrictions (e.g. the schema of the tables that a stored procedure reads must be fixed, the stored procedures must execute within a single transaction). To overcome some of these restrictions, SQL Server allows regular/unrestricted/interpreted stored procedures to read and write Hekaton tables.</p>
<h2 id="transaction-management">Transaction Management</h2>
<p>Hekaton supports snapshot isolation, repeatable read, and serializability all implemented with multiversion concurrency control. There are two conditions which can be checked during validation:</p>
<ol style="list-style-type: decimal">
<li><strong>Read stability</strong>. All the versions that a transaction read must still be valid versions upon commit.</li>
<li><strong>Phantom avoidance</strong>. All the scans a transaction made must be repeatable upon commit.</li>
</ol>
<p>Checking both these conditions guarantees serializability. Checking read stability guarantees repeatable read. Checking neither guarantees snapshot isolation.</p>
<p>Each transaction is assigned a read timestamp (Hekaton uses the begin timestamp as the read timestamp) and a commit timestamp. To check the conditions above, transactions maintain a <strong>read set</strong> pointing to all the read versions and <strong>scan set</strong> describing how to repeat scans. Upon commit, the transaction verifies that the read set is still valid as of the commit timestamp, and it re-executes scans to make sure that no versions have been added, deleted, or updated.</p>
<p>If a transaction reads something from a yet to be committed transaction, it marks the transactions as a <strong>commit dependency</strong>. Cascaded aborts are possible because if the commit dependency aborts, the dependent transaction must also abort. Moreover, a transaction must wait for all commit dependencies to commit before it does.</p>
<p>After a transaction commits, it updates the end timestamp of all the tuples it deleted and the begin timestamp of all the tuples it inserted. To do so efficiently, transactions maintain a <strong>write set</strong> pointing to these tuples.</p>
<h2 id="transaction-durability">Transaction Durability</h2>
<p>Hekaton stores two types of data for recovery:</p>
<ol style="list-style-type: decimal">
<li><strong>Log streams.</strong> A log stream records all of the tuple versions inserted and deleted by an transaction during a period of logical time.</li>
<li><strong>Checkpoint streams.</strong> There are two types of checkpoint streams. A <strong>data stream</strong> records all of the tuples inserted during a period of logical time. Every data stream has a corresponding <strong>delta stream</strong> which records the version ids of all the deleted tuples during the same period of logical time.</li>
</ol>
<p>Each transaction produces a single record in the log stream once it commits. This is possible because Hekaton does not use write-ahead logging; it does not need to flush log entries to disk before flushing pages to disk because it doesn't flush pages to disk. Remember that everything is kept in memory. Hekaton also batches log entries together to improve IO efficiency.</p>
<p>Periodically, parts of the log are converted into data and delta streams and flushed to disk. Even more periodically, data and delta streams are merged together into more compressed streams that cover a larger period of logical time.</p>
<p>Upon recovery, Hekaton processes data/delta stream pairs in parallel.</p>
<p>Note that index operations are not logged at all; indexes are rebuilt during execution. This pushes the bulk recovery overhead to recovery time.</p>
<h2 id="garbage-collection">Garbage Collection</h2>
<p>A tuple version is garbage in one of two scenarios:</p>
<ol style="list-style-type: decimal">
<li>A tuple version was deleted at time <code>t</code> and every pending transaction has a read timestamp later than <code>t</code>.</li>
<li>A tuple version was created by a transaction which rolled back.</li>
</ol>
<p>Garbage collection deletes both kinds of garbage. Hekaton's garbage collector has two kinds of collection: online and offline.</p>
<ol style="list-style-type: decimal">
<li><strong>Online.</strong> Whenever a transaction scans through the entries of an index, they are free to remove any tuple versions that are garbage. If a tuple version is removed from all indexes, it is reclaimed. This online collection piggybacks off the existing cost of scanning indexes. Moreover, it ensures that hot indexes are kept clean.</li>
<li><strong>Offline.</strong> Periodically, the garbage collector has to clean out the <em>dusty corners</em> of indexes that are not cleaned online. This work is parallelized across all cores. A worker thread will alternate between processing transactions and performing a small amount of garbage collection. This mechanism throttles processing to ensure that garbage does not endlessly pile up.</li>
</ol>
  </div>

  <script type="text/javascript" src="../js/mathjax_config.js"></script>
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-90310997-2', 'auto');
		ga('send', 'pageview');
	</script>
</body>
</html>
